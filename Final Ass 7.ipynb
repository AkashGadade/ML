{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407b4300",
   "metadata": {},
   "source": [
    "Use different voting mechanism and Apply AdaBoost (Adaptive\n",
    "Boosting), Gradient Tree Boosting (GBM), XGBoost classification on Iris\n",
    "dataset and compare the performance of three models using different\n",
    "evaluation measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f451f1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboostNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading xgboost-2.0.1-py3-none-win_amd64.whl (99.7 MB)\n",
      "     ---------------------------------------- 99.7/99.7 MB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in g:\\user\\lib\\site-packages (from xgboost) (1.10.0)\n",
      "Requirement already satisfied: numpy in g:\\user\\lib\\site-packages (from xgboost) (1.26.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.1\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fef0fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ba361",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "287ae82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoost\n",
      "Accuracy: 0.93\n",
      "Precision: 0.94\n",
      "Recall: 0.93\n",
      "F1 Score: 0.93\n",
      "Confusion Matrix:\n",
      "[[23  0  0]\n",
      " [ 0 19  0]\n",
      " [ 0  4 14]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create and train the AdaBoost Classifier\n",
    "adaboost = AdaBoostClassifier(n_estimators=30, random_state=0)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Model: AdaBoost')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'Confusion Matrix:\\n{confusion}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cd63ab",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e582afa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Gradient Boosting\n",
      "Accuracy: 0.98\n",
      "Precision: 0.98\n",
      "Recall: 0.98\n",
      "F1 Score: 0.98\n",
      "Confusion Matrix:\n",
      "[[23  0  0]\n",
      " [ 0 19  0]\n",
      " [ 0  1 17]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Create and train the Gradient Boosting Classifier\n",
    "gradboost = GradientBoostingClassifier(n_estimators=40, random_state=0)\n",
    "gradboost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gradboost.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Model: Gradient Boosting')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'Confusion Matrix:\\n{confusion}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c255af13",
   "metadata": {},
   "source": [
    "# XGBoost Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8bab2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBoost\n",
      "Accuracy: 0.98\n",
      "Precision: 0.98\n",
      "Recall: 0.98\n",
      "F1 Score: 0.98\n",
      "Confusion Matrix:\n",
      "[[23  0  0]\n",
      " [ 0 19  0]\n",
      " [ 0  1 17]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Create and train the XGBoost Classifier\n",
    "xgboost = XGBClassifier(n_estimators=50, random_state=0)\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgboost.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Model: XGBoost')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'Confusion Matrix:\\n{confusion}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c897034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd1516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07dcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca54e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eabb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a3fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5f03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4301866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9dae03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1642ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a6fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57571c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af050e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60cae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800806a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dbc529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28b328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15476932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b5c6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf689fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f674399",
   "metadata": {},
   "source": [
    "Certainly! Here's some theoretical information about the three ensemble learning models: AdaBoost, Gradient Boosting, and XGBoost:\n",
    "\n",
    "**AdaBoost (Adaptive Boosting):**\n",
    "\n",
    "1. **Basic Concept:** AdaBoost is an ensemble learning method that combines multiple weak learners (typically decision trees) to create a strong learner. It focuses on the misclassified samples and gives them more weight in each iteration.\n",
    "\n",
    "2. **Algorithm Overview:** The algorithm iteratively trains weak learners, assigns weights to misclassified samples, and combines the weak learners' predictions. It gives more weight to the samples that were misclassified in the previous iteration.\n",
    "\n",
    "3. **Strengths:** AdaBoost is simple to implement, resistant to overfitting, and often provides good results for a wide range of classification problems. It's particularly effective when combined with weak learners.\n",
    "\n",
    "4. **Weaknesses:** AdaBoost can be sensitive to noisy data and outliers, and it may not perform well with highly complex data.\n",
    "\n",
    "**Gradient Boosting:**\n",
    "\n",
    "1. **Basic Concept:** Gradient Boosting is another ensemble learning method that builds an additive model by fitting weak learners in a stage-wise manner. It minimizes the loss function's gradient at each step to update the model.\n",
    "\n",
    "2. **Algorithm Overview:** Gradient Boosting starts with an initial prediction and then fits a weak learner to the residuals (the difference between the actual values and the current predictions). This process is repeated for a specified number of iterations.\n",
    "\n",
    "3. **Strengths:** Gradient Boosting is powerful and highly flexible, capable of fitting complex data. It can handle various loss functions and is generally robust to outliers.\n",
    "\n",
    "4. **Weaknesses:** It can be computationally expensive and requires careful tuning of hyperparameters. Like AdaBoost, it may not perform well with noisy data.\n",
    "\n",
    "**XGBoost (Extreme Gradient Boosting):**\n",
    "\n",
    "1. **Basic Concept:** XGBoost is an optimized and efficient implementation of Gradient Boosting. It incorporates several enhancements to make it faster and more robust, making it a popular choice for structured data problems.\n",
    "\n",
    "2. **Algorithm Overview:** XGBoost is based on Gradient Boosting but includes features like parallel processing, regularization, and more advanced tree building. It uses a weighted quantile sketch for finding the best splits, which speeds up the training process.\n",
    "\n",
    "3. **Strengths:** XGBoost is known for its speed and performance. It's highly optimized, allows for parallel processing, handles missing data efficiently, and has a wide range of hyperparameters for fine-tuning.\n",
    "\n",
    "4. **Weaknesses:** It may require some experience and experimentation to choose the best hyperparameters for a specific problem. It may not be as suitable for problems with a small number of samples.\n",
    "\n",
    "These three ensemble models, AdaBoost, Gradient Boosting, and XGBoost, are valuable tools in machine learning, each with its unique characteristics and use cases. The choice of model depends on the specific problem and data you're working with, as well as the trade-offs you're willing to make in terms of computational complexity and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caddc45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
